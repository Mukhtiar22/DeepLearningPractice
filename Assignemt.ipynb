{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1oMkG0XS1gdr"
   },
   "source": [
    "# ACTIVATION FUNCTIONS\n",
    "## 1. Step Function\n",
    "\n",
    "The Step function outputs 1 for positive input and 0 for negative input.\n",
    "\n",
    "Purpose: Used in early perceptrons for binary classification.\n",
    "\n",
    "Drawback: Non-differentiable at 0 and provides no gradient information → not used in modern deep learning.\n",
    "\n",
    "Derivative: 0 almost everywhere → cannot update weights properly.\n",
    "\n",
    "## 2. Sigmoid Function\n",
    "\n",
    "The Sigmoid squashes input to range (0, 1), making it interpretable as probability.\n",
    "\n",
    "Purpose: Popular for binary classification, logistic regression.\n",
    "\n",
    "Pros: Smooth, differentiable, outputs probability.\n",
    "\n",
    "Cons: Causes vanishing gradients for very large or small inputs (saturation).\n",
    "\n",
    "Derivative: σ(x) * (1 - σ(x)), used during backpropagation.\n",
    "\n",
    "## 3. ReLU (Rectified Linear Unit)\n",
    "\n",
    "ReLU outputs x for positive inputs and 0 otherwise.\n",
    "\n",
    "Purpose: Most widely used activation in deep networks.\n",
    "\n",
    "Pros: Computationally cheap, reduces vanishing gradient problem.\n",
    "\n",
    "Cons: Can suffer from dying ReLU problem (neurons stuck at 0 forever).\n",
    "\n",
    "Derivative: 1 for positive inputs, 0 otherwise → very simple for backpropagation.\n",
    "\n",
    "## 4. Leaky ReLU\n",
    "\n",
    "Leaky ReLU modifies ReLU by allowing a small slope αx for negative inputs.\n",
    "\n",
    "Purpose: Solves dying ReLU by letting small gradient flow even for x < 0.\n",
    "\n",
    "Derivative: 1 for positive inputs, α for negative inputs.\n",
    "\n",
    "## 5. PReLU (Parametric ReLU)\n",
    "\n",
    "PReLU generalizes Leaky ReLU by making α learnable during training.\n",
    "\n",
    "Purpose: Model can adapt negative slope to improve performance.\n",
    "\n",
    "Benefit: More flexible and can achieve better accuracy on some tasks.\n",
    "\n",
    "## 6. ELU (Exponential Linear Unit)\n",
    "\n",
    "ELU uses an exponential curve for negative values, instead of constant slope.\n",
    "\n",
    "Purpose: Brings negative outputs closer to 0, reducing bias shift and improving learning.\n",
    "\n",
    "Pros: Smooth gradient for negative inputs → better convergence.\n",
    "\n",
    "Cons: Slightly slower due to exponential calculation.\n",
    "\n",
    "## 7. Swish\n",
    "\n",
    "Swish is defined as x * sigmoid(x).\n",
    "\n",
    "Purpose: A newer activation function that is smooth and non-monotonic.\n",
    "\n",
    "Advantage: Often outperforms ReLU in deep networks because it allows small negative values.\n",
    "\n",
    "Derivative: Combines sigmoid + product rule, slightly more complex but smooth.\n",
    "\n",
    "## 8. Linear Activation\n",
    "\n",
    "Linear activation is simply f(x) = x.\n",
    "\n",
    "Purpose: Used in regression tasks (output layer).\n",
    "\n",
    "Derivative: Constant 1 → simple backpropagation.\n",
    "\n",
    "Limitation: Cannot model non-linear relationships by itself.\n",
    "\n",
    "## 9. Tanh\n",
    "\n",
    "Tanh squashes input to (-1, 1) range.\n",
    "\n",
    "Purpose: Similar to sigmoid but zero-centered → better for training.\n",
    "\n",
    "Pros: Better convergence than sigmoid for many tasks.\n",
    "\n",
    "Cons: Still suffers from vanishing gradients for large |x|.\n",
    "\n",
    "## 10. Softmax\n",
    "\n",
    "Softmax converts a vector of values into probabilities that sum to 1.\n",
    "\n",
    "Purpose: Used in the final layer for multi-class classification.\n",
    "\n",
    "Derivative: Produces a Jacobian matrix used in backprop for cross-entropy loss.\n",
    "\n",
    "Note: Applied across all classes, not element-wise like others."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "fqicYD820say",
    "outputId": "b7c31eba-9346-4267-e5ac-a98c7f47a8fa",
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "x = np.linspace(-10, 10, 100)\n",
    "\n",
    "\n",
    "def step(x):\n",
    "    return np.where(x >= 0, 1, 0)\n",
    "\n",
    "def step_derivative(x):\n",
    "    return np.zeros_like(x)  # derivative undefined at 0, assume 0\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    s = sigmoid(x)\n",
    "    return s * (1 - s)\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return np.where(x > 0, 1, 0)\n",
    "\n",
    "def leaky_relu(x, alpha=0.01):\n",
    "    return np.where(x > 0, x, alpha * x)\n",
    "\n",
    "def leaky_relu_derivative(x, alpha=0.01):\n",
    "    return np.where(x > 0, 1, alpha)\n",
    "\n",
    "def prelu(x, alpha=0.25):\n",
    "    return np.where(x > 0, x, alpha * x)\n",
    "\n",
    "def prelu_derivative(x, alpha=0.25):\n",
    "    return np.where(x > 0, 1, alpha)\n",
    "\n",
    "def elu(x, alpha=1.0):\n",
    "    return np.where(x > 0, x, alpha * (np.exp(x) - 1))\n",
    "\n",
    "def elu_derivative(x, alpha=1.0):\n",
    "    return np.where(x > 0, 1, elu(x, alpha) + alpha)\n",
    "\n",
    "def swish(x):\n",
    "    return x * sigmoid(x)\n",
    "\n",
    "def swish_derivative(x):\n",
    "    s = sigmoid(x)\n",
    "    return s + x * s * (1 - s)\n",
    "\n",
    "def linear(x):\n",
    "    return x\n",
    "\n",
    "def linear_derivative(x):\n",
    "    return np.ones_like(x)\n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x))\n",
    "    return exp_x / np.sum(exp_x)\n",
    "\n",
    "def softmax_derivative(x):\n",
    "    s = softmax(x)\n",
    "    return np.diag(s) - np.outer(s, s)\n",
    "\n",
    "def tanh_fn(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def tanh_derivative(x):\n",
    "    return 1 - np.tanh(x)**2\n",
    "\n",
    "\n",
    "def plot_activation_and_derivative(func, dfunc, name, is_vector=False):\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    y = func(x)\n",
    "    dy = dfunc(x)\n",
    "\n",
    "    axs[0].plot(x, y, label=name)\n",
    "    axs[0].set_title(f\"{name} Activation\")\n",
    "    axs[0].grid()\n",
    "    axs[0].legend()\n",
    "\n",
    "    axs[1].plot(x, dy, label=f\"d{name}/dx\", color='orange')\n",
    "    axs[1].set_title(f\"{name} Derivative\")\n",
    "    axs[1].grid()\n",
    "    axs[1].legend()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_activation_and_derivative(step, step_derivative, \"Step\")\n",
    "plot_activation_and_derivative(sigmoid, sigmoid_derivative, \"Sigmoid\")\n",
    "plot_activation_and_derivative(relu, relu_derivative, \"ReLU\")\n",
    "plot_activation_and_derivative(lambda x: leaky_relu(x, 0.01), lambda x: leaky_relu_derivative(x, 0.01), \"Leaky ReLU\")\n",
    "plot_activation_and_derivative(lambda x: prelu(x, 0.25), lambda x: prelu_derivative(x, 0.25), \"PReLU\")\n",
    "plot_activation_and_derivative(lambda x: elu(x, 1.0), lambda x: elu_derivative(x, 1.0), \"ELU\")\n",
    "plot_activation_and_derivative(swish, swish_derivative, \"Swish\")\n",
    "plot_activation_and_derivative(linear, linear_derivative, \"Linear\")\n",
    "plot_activation_and_derivative(tanh_fn, tanh_derivative, \"Tanh\")\n",
    "\n",
    "\n",
    "softmax_y = softmax(x)\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(x, softmax_y, label='Softmax Output')\n",
    "plt.title(\"Softmax Activation over Input Vector\")\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv (3.12.4)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
